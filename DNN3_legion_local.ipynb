{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SURE_DNN3_legion_local.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asilkoc/Complete-Python-3-Bootcamp/blob/master/DNN3_legion_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GItcbakuLCLU",
        "outputId": "185f19f9-04bb-4203-bbbe-7ceaff50f304"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA95ZIpgNi71"
      },
      "source": [
        "#%%\n",
        "import math\n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from numba import jit, prange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6zkQwOANkZd",
        "outputId": "03c4b900-279a-40e8-ad14-fa9c52af8e3b"
      },
      "source": [
        "data_1 = pd.read_csv('/content/drive/My Drive/data/test_K_6_Pt_40_debug_1.csv', header=None).values\n",
        "data_2 = pd.read_csv('/content/drive/My Drive/data/test_K_6_Pt_40_debug_2.csv', header=None).values\n",
        "data_3 = pd.read_csv('/content/drive/My Drive/data/test_K_6_Pt_40_i7_cpu_conflict.csv',error_bad_lines=False,header=None).values\n",
        "data_4 = pd.read_csv('/content/drive/My Drive/data/test_K_6_Pt_40_AMD.csv',header=None).values\n",
        "data_5 = pd.read_csv('/content/drive/My Drive/data/test_K_6_Pt_40_AMD_2.csv',header=None).values\n",
        "\n",
        "data_1_array = np.array(data_1)\n",
        "data_2_array = np.array(data_2)\n",
        "data_3_array = np.array(data_3)\n",
        "data_4_array = np.array(data_4)\n",
        "data_5_array = np.array(data_5)\n",
        "\n",
        "# get rid of the speical five user cases (due to the construction of the csv file)\n",
        "data_1_array = data_1_array[6:69999]\n",
        "data_2_array = data_2_array[6:69999]\n",
        "data_3_array = data_3_array[6:23179,:37]\n",
        "data_4_array = data_4_array[6:139999]\n",
        "data_5_array = data_5_array[6:69999]\n",
        "\n",
        "del data_1\n",
        "del data_2\n",
        "del data_3\n",
        "del data_4\n",
        "del data_5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y2RjM6JNmhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5d4073-a47b-4deb-b932-1629a37d2394"
      },
      "source": [
        "#%% replace all 'i' with 'j'\n",
        "for row in range(len(data_5_array)):\n",
        "    for colomn in range(len(data_5_array[1,:])):\n",
        "        if type(data_5_array[row][colomn]) == str:\n",
        "            data_5_array[row][colomn] = data_5_array[row][colomn].replace('i','j')\n",
        "\n",
        "for row in range(len(data_4_array)):\n",
        "    for colomn in range(len(data_4_array[1,:])):\n",
        "        if type(data_4_array[row][colomn]) == str:\n",
        "            data_4_array[row][colomn] = data_4_array[row][colomn].replace('i','j')   \n",
        "\n",
        "del row\n",
        "del colomn\n",
        "            \n",
        "#%%\n",
        "#concatenate two array into one\n",
        "data_total_array_1 = np.concatenate((data_1_array,data_2_array),axis=0)\n",
        "data_total_array_2 = np.concatenate((data_3_array,data_4_array),axis=0)\n",
        "data_total_array_3 = np.concatenate((data_total_array_1,data_total_array_2),axis=0)\n",
        "data_total_array = np.concatenate((data_total_array_3,data_5_array), axis=0)\n",
        "\n",
        "del data_total_array_1\n",
        "del data_total_array_2\n",
        "del data_total_array_3\n",
        "\n",
        "del data_1_array\n",
        "del data_2_array\n",
        "del data_3_array\n",
        "del data_4_array\n",
        "del data_5_array\n",
        "\n",
        "\n",
        "#start = torch.cuda.Event(enable_timing=True)\n",
        "#end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "#%% dataset perprocessing\n",
        "# checking if data is not following the format (i.e. not 7 rows as a cluster dataset)\n",
        "x_list = []\n",
        "x_temp = 0.0\n",
        "y_list = []\n",
        "cluster_start_ind = 0\n",
        "cluster_end_ind = 0\n",
        "prev_nan_ind = 0\n",
        "cluster_length = 0\n",
        "isNanStartBoolean = False\n",
        "isNanNextBoolean = False\n",
        "for i in range(data_total_array.shape[0]):\n",
        "    if pd.isna(data_total_array[i][36]):\n",
        "        cluster_end_ind = i\n",
        "        prev_nan_ind = i\n",
        "        cluster_length = cluster_end_ind - cluster_start_ind\n",
        "        if cluster_length == 6:\n",
        "            x_list.append(data_total_array[cluster_start_ind:cluster_end_ind,:36].tolist())\n",
        "            y_list.append(data_total_array[cluster_start_ind:cluster_end_ind,36].tolist())\n",
        "        cluster_start_ind = i+1\n",
        "    \n",
        "# transfer all complex number str to complex and seperate them into real and complex numbers\n",
        "for m in range(len(x_list)):\n",
        "    for n in range(6):\n",
        "        for h in range(36):\n",
        "            \n",
        "            if x_list[m][n][h].count('j') > 1 or x_list[m][n][h].count('e') > 2:\n",
        "                #print(x_list[m][n][h])\n",
        "                x_list[m][n][h] = '0'\n",
        "            \n",
        "            x_temp = complex(x_list[m][n][h]).imag\n",
        "            x_list[m][n][h] = complex(x_list[m][n][h]).real\n",
        "            x_list[m][n].append(x_temp)\n",
        "            \n",
        "for m in range(len(y_list)):\n",
        "    for n in range(6):\n",
        "        y_list[m][n] = complex(y_list[m][n]).real\n",
        "\n",
        "y_array = np.array(y_list)\n",
        "x_array = np.array(x_list)        \n",
        "\n",
        "\n",
        "del x_list\n",
        "del y_list\n",
        "\n",
        "data_tensor_final_train = torch.utils.data.TensorDataset(torch.Tensor(x_array), torch.Tensor(y_array))\n",
        "\n",
        "del x_array\n",
        "del y_array\n",
        "del data_total_array\n",
        "del m, n, h, i, isNanNextBoolean, isNanStartBoolean, cluster_end_ind, cluster_length, cluster_start_ind, prev_nan_ind, x_temp\n",
        "    \n",
        "#%% data spliting \n",
        "'''\n",
        "DNN_trainset \n",
        "DNN_valset\n",
        "DNN_testset\n",
        "'''\n",
        "DNN_trainset, DNN_valset = torch.utils.data.random_split(data_tensor_final_train, lengths=[int(len(data_tensor_final_train)*0.9), len(data_tensor_final_train)-int(len(data_tensor_final_train)*0.9)])\n",
        "train_data = torch.utils.data.DataLoader(DNN_trainset, batch_size=10, shuffle=True)\n",
        "val_data = torch.utils.data.DataLoader(DNN_valset, batch_size=10, shuffle=False)\n",
        "\n",
        "del DNN_trainset\n",
        "del DNN_valset\n",
        "\n",
        "\n",
        "#%% constructing the model\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden_1, n_hidden_2, n_hidden_3):\n",
        "        super(DNN, self).__init__() # nitialing the super class \n",
        "        # n_in is the number of input, n_out is the number of output\n",
        "        self.layer1 = nn.Linear(n_in, n_hidden_1) # fully connectted neural network \n",
        "        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
        "        self.layer3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
        "        self.layer4 = nn.Linear(n_hidden_3, 6) \n",
        "    \n",
        "    # self.layer1(x) -> wx+b \n",
        "    # torch.relu() -> activition\n",
        "    # self.layer2() -> wx+b \n",
        "    #'''\n",
        "    #def forward(self, x):\n",
        "     #   return self.layer4(self.sigmoid(self.layer3(torch.relu(self.layer2(torch.relu(self.layer1(x)))))))\n",
        "    #'''\n",
        "    def forward(self,x):\n",
        "        return torch.sigmoid(self.layer4(torch.sigmoid(self.layer3(torch.sigmoid(self.layer2(torch.sigmoid(self.layer1(x))))))))\n",
        "    \n",
        "    \n",
        "    #def sigmoid(x):\n",
        "     #  return 1 / (1+torch.exp(x))\n",
        "    \n",
        "#%%\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SftwO8cR1WXX"
      },
      "source": [
        "#%% defining loss functions\n",
        "#####################################################################################################\n",
        "#                                       log cosh loss                                               #\n",
        "#####################################################################################################\n",
        "def logcosh(pred, true):\n",
        "    loss = torch.log(torch.cosh(true - pred))\n",
        "    return torch.sum(loss)\n",
        "\n",
        "\n",
        "#%%\n",
        "#####################################################################################################\n",
        "#                                       sum_rate_loss with tensor                                   #\n",
        "#####################################################################################################\n",
        "@jit\n",
        "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
        "    #SUM_RATE_valid_list = []\n",
        "    #SUM_RATE_pred_list = []\n",
        "    temp_tensor_valid = torch.empty(1).to(device)\n",
        "    temp_tensor_valid_check = temp_tensor_valid\n",
        "    temp_tensor_pred = torch.empty(1).to(device)\n",
        "    temp_tensor_pred_check = temp_tensor_pred\n",
        "    for rows in prange(len(H_eff_tensor)):\n",
        "            H_eff = H_eff_tensor[rows,:,36:72].type(torch.complex64)*1j+H_eff_tensor[rows,:,:36]\n",
        "            P_label = P_label_tensor[rows]\n",
        "            P_pred = P_pred_tensor[rows]\n",
        "            #------------------------------------ initialization -----------------------------------------#\n",
        "            # initialization\n",
        "            K               = torch.tensor(len(H_eff)).to(device)            # number of rows \n",
        "            NRF             = torch.tensor(len(H_eff[0])).to(device)          # number of columns\n",
        "            P_opt           = torch.diagflat(P_label);\n",
        "            P_pred          = torch.diagflat(P_pred);\n",
        "            \n",
        "            #------------------------------------ P_label start -----------------------------------------#\n",
        "            # calculate BB precoder for P_label\n",
        "            # there was a sum on NRF (sum(NRF))\n",
        "            W               = torch.linalg.pinv(torch.matmul(torch.transpose(torch.conj(H_eff),0,1),H_eff) + K*alpha*(torch.eye(torch.sum(NRF)).to(device)))\n",
        "            B               = torch.matmul(W,torch.transpose(torch.conj(H_eff),0,1))\n",
        "            Eps             = torch.sqrt(P_t/diag_sum(torch.matmul(torch.matmul(B,P_opt) ,(torch.transpose(torch.conj(torch.matmul(B,P_opt)),0,1)))))                       \n",
        "            B_All           = torch.matmul(Eps*B,P_opt)\n",
        "            \n",
        "             \n",
        "            # calculate sum rate for P_label\n",
        "            H_all           = torch.matmul(H_eff,B_All)\n",
        "            GAIN            = torch.square(torch.abs((H_all))) \n",
        "            SIGNAL          = torch.diag(GAIN)\n",
        "            SIGNAL          = SIGNAL.reshape(len(P_pred),1)\n",
        "            INTERF          = torch.sum(GAIN-torch.diagflat(torch.diag(GAIN)),axis=1)\n",
        "            SUM_RATE_valid  = torch.sum(torch.log2(1+SIGNAL/(INTERF + NoisePower).reshape(len(P_pred),1)))\n",
        "            #SUM_RATE_valid_list.append(SUM_RATE_valid)\n",
        "            if (temp_tensor_valid == temp_tensor_valid_check)[0]:\n",
        "                temp_tensor_valid = SUM_RATE_valid\n",
        "                temp_tensor_valid = temp_tensor_valid.reshape(1)\n",
        "            else:\n",
        "                temp_tensor_valid = torch.cat((temp_tensor_valid.to(device),SUM_RATE_valid.reshape(1)),0)\n",
        "            #------------------------------------ P_pred start -----------------------------------------#\n",
        "            \n",
        "            # calculate sun rate for P_pred\n",
        "            W               = torch.linalg.pinv(torch.matmul(torch.transpose(torch.conj(H_eff),0,1),H_eff) + K*alpha*(torch.eye(torch.sum(NRF)).to(device)))\n",
        "            B               = torch.matmul(W,torch.transpose(torch.conj(H_eff),0,1))\n",
        "            Eps             = torch.sqrt(P_t/diag_sum(torch.matmul(torch.matmul(B,P_pred) ,(torch.transpose(torch.conj(torch.matmul(B,P_pred)),0,1))))+ 1e-8) + 1e-14                          \n",
        "            B_All           = torch.matmul(Eps*B,P_pred)\n",
        "            \n",
        "            \n",
        "            # Calculate Sum-Rate for P_pred\n",
        "            H_all           = torch.matmul(H_eff,B_All)\n",
        "            GAIN            = torch.square(torch.abs((H_all)))\n",
        "            SIGNAL          = torch.diag(GAIN).reshape(len(P_pred),1)\n",
        "            INTERF          = torch.sum(GAIN-torch.diagflat(torch.diag(GAIN)),axis=1)\n",
        "            SUM_RATE_pred  = torch.sum(torch.log2(1+SIGNAL/(INTERF + NoisePower).reshape(len(P_pred),1)))\n",
        "            #SUM_RATE_pred_list.append(SUM_RATE_pred)\n",
        "            if (temp_tensor_pred == temp_tensor_pred_check)[0]:\n",
        "                temp_tensor_pred = SUM_RATE_pred\n",
        "                temp_tensor_pred = temp_tensor_pred.reshape(1)\n",
        "            else: \n",
        "                temp_tensor_pred = torch.cat((temp_tensor_pred.to(device),SUM_RATE_pred.reshape(1)),0)\n",
        "            \n",
        "    #--------------------------------- sum rate loss start -------------------------------------#\n",
        "    \n",
        "    # Calculate Loss\n",
        "    #LOSS = SUM_RATE_valid - SUM_RATE_pred;\n",
        "    #LOSS = nn.MSELoss(SUM_RATE_valid,SUM_RATE_pred)\n",
        "    return temp_tensor_valid.to(device), temp_tensor_pred.to(device)\n",
        "    \n",
        "    \n",
        "    \n",
        "def db2pow(ydb):\n",
        "    y = 10**(ydb/10)\n",
        "    return y\n",
        "    \n",
        "def diag_sum(H):\n",
        "    for i in range(len(H)):\n",
        "        if i == 0:\n",
        "            sum_temp = H[i][i]\n",
        "        if i > 0:\n",
        "            sum_temp += H[i][i]\n",
        "    return sum_temp    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikGZ9ITY1WpJ"
      },
      "source": [
        "#%%\n",
        "model = DNN(n_in=6*72, n_hidden_1=512, n_hidden_2=1024, n_hidden_3 =512).to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "#loss_func = nn.L1Loss()\n",
        "#loss_func = nn.SmoothL1Loss(beta=1.0)\n",
        "#loss_func = logcosh()\n",
        "val_loss_func = nn.MSELoss()\n",
        "#val_loss_func = nn.L1Loss(reduction='sum')\n",
        "#val_loss_func = nn.SmoothL1Loss(reduction='sum',beta=1.0)\n",
        "#val_loss_func = logcosh()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0000001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPbddG7Z1XHJ",
        "outputId": "68ef16b8-fe70-4858-b6a2-36642cee7b2d"
      },
      "source": [
        "#%%\n",
        "'''\n",
        "trainning loop\n",
        "'''\n",
        "#torch.autograd.set_detect_anomaly(False)\n",
        "counter = 0\n",
        "n_epoch = 100\n",
        "model = model.to(device)\n",
        "y_hat_max = []\n",
        "y_max = []\n",
        "batch_loss_all = []\n",
        "val_loss_all = []\n",
        "batch_loss_min = 100\n",
        "NoisePower_dBm  = -134;\n",
        "NoisePower      = torch.tensor(db2pow(NoisePower_dBm - 30)).to(device);\n",
        "P_t_dBm         = 40;\n",
        "P_t             = torch.tensor(db2pow(P_t_dBm - 30)).to(device);\n",
        "alpha           = torch.tensor(NoisePower/P_t).to(device);\n",
        "#start.record()\n",
        "for epoch in range(n_epoch): \n",
        "    batch_loss_accum = 0\n",
        "    model.train()\n",
        "    for x, y in train_data: \n",
        "        # remember to to(device first)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        '''\n",
        "        print(\"---------------model -------------------\")\n",
        "        print(model.state_dict())\n",
        "        '''\n",
        "        y_hat = model(x.view(-1, 6*72))\n",
        "        '''\n",
        "        print(\"--------------y_hat---------------\")\n",
        "        print(y_hat)\n",
        "        '''\n",
        "        [SUM_RATE_valid, SUM_RATE_pred] = Sum_rate_loss_tensor(x, y.type(torch.complex64), y_hat.type(torch.complex64),NoisePower,P_t,alpha)\n",
        "        '''\n",
        "        print(\"-------------- Sum rate pred -------------\")\n",
        "        print(SUM_RATE_pred)\n",
        "        '''\n",
        "        batch_loss = loss_func(SUM_RATE_valid,SUM_RATE_pred)+ 1e-14 \n",
        "        batch_loss_all.append(batch_loss.cpu().detach().numpy())\n",
        "        '''\n",
        "        print(\"-------------- batch loss --------------\")\n",
        "        print(batch_loss)\n",
        "        '''\n",
        "        #batch_loss_all.append(batch_loss)\n",
        "        batch_loss.backward() # autodiff\n",
        "        '''\n",
        "        print(\"-----------------------------------------------------------\")\n",
        "        print(\"layer 1 weight gradient: \" + str(model.layer1.weight.grad))\n",
        "        print(\"Layer 1 bias gradident: \"+ str(model.layer1.bias.grad))\n",
        "        print(\"layer 2 weight gradient: \" + str(model.layer2.weight.grad))\n",
        "        print(\"Layer 2 bias gradident: \"+ str(model.layer2.bias.grad))\n",
        "        print(\"layer 3 weight gradient: \" + str(model.layer3.weight.grad))\n",
        "        print(\"Layer 3 bias gradident: \"+ str(model.layer3.bias.grad))\n",
        "        print(\"layer 4 weight gradient: \" + str(model.layer4.weight.grad))\n",
        "        print(\"Layer 4 bias gradident: \"+ str(model.layer4.bias.grad))\n",
        "        '''\n",
        "        counter += 1\n",
        "        optimizer.step()\n",
        "    \n",
        "      \n",
        "\n",
        "\n",
        "    model.eval() # signal evaluation phase\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        val_count = 0\n",
        "        for x, y in val_data:\n",
        "            x, y = x.to(device), y.to(device) # move the data to cuda\n",
        "            y_hat = model(x.view(-1,6*72)) # resize the size of the x, -1 depends on the batch size\n",
        "            # initialization\n",
        "            [SUM_RATE_valid, SUM_RATE_pred] = Sum_rate_loss_tensor(x, y.type(torch.complex64), y_hat.type(torch.complex64),NoisePower,P_t,alpha)\n",
        "            val_loss += val_loss_func(SUM_RATE_pred,SUM_RATE_valid)\n",
        "            val_count += 1\n",
        "        val_loss = val_loss/val_count\n",
        "        val_loss_all.append(val_loss.cpu().detach().numpy())\n",
        "\n",
        "    # finding the optimal output\n",
        "    if len(y_hat_max) == 0:\n",
        "      y_hat_max = y_hat\n",
        "      val_loss_min = val_loss\n",
        "      y_max = y\n",
        "    if len(y_hat_max) > 0:\n",
        "      if val_loss < val_loss_min:\n",
        "        y_hat_max = y_hat\n",
        "        val_loss_min = val_loss\n",
        "        y_max = y\n",
        "    #PATH = \"optimal_model_epoch_\"+ str(epoch) + \".pt\"\n",
        "    #torch.save(model,PATH)\n",
        "\n",
        "    print(\"-----------%d\\tbatch-loss: %.4f\\tval-loss: %.4f\"%(epoch, batch_loss, val_loss))\n",
        "#end.record()\n",
        "# Waits for everything to finish running\n",
        "#torch.cuda.synchronize()\n",
        "\n",
        "#print(\"time used: {}\".format(start.elapsed_time(end))) # time here is in ms \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "<ipython-input-37-a0bf00e11f4d>:14: NumbaWarning: \n",
            "Compilation is falling back to object mode WITH looplifting enabled because Function \"Sum_rate_loss_tensor\" failed type inference due to: Untyped global name 'diag_sum': cannot determine Numba type of <class 'function'>\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 38:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "            B               = torch.matmul(W,torch.transpose(torch.conj(H_eff),0,1))\n",
            "            Eps             = torch.sqrt(P_t/diag_sum(torch.matmul(torch.matmul(B,P_opt) ,(torch.transpose(torch.conj(torch.matmul(B,P_opt)),0,1)))))                       \n",
            "            ^\n",
            "\n",
            "  @jit\n",
            "<ipython-input-37-a0bf00e11f4d>:14: NumbaWarning: \n",
            "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"Sum_rate_loss_tensor\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 22:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "    temp_tensor_pred_check = temp_tensor_pred\n",
            "    for rows in prange(len(H_eff_tensor)):\n",
            "    ^\n",
            "\n",
            "  @jit\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"Sum_rate_loss_tensor\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 18:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "    #SUM_RATE_pred_list = []\n",
            "    temp_tensor_valid = torch.empty(1).to(device)\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
            "\n",
            "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 18:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "    #SUM_RATE_pred_list = []\n",
            "    temp_tensor_valid = torch.empty(1).to(device)\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n",
            "<ipython-input-37-a0bf00e11f4d>:14: NumbaWarning: \n",
            "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"Sum_rate_loss_tensor\" failed type inference due to: Untyped global name 'diag_sum': cannot determine Numba type of <class 'function'>\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 38:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "            B               = torch.matmul(W,torch.transpose(torch.conj(H_eff),0,1))\n",
            "            Eps             = torch.sqrt(P_t/diag_sum(torch.matmul(torch.matmul(B,P_opt) ,(torch.transpose(torch.conj(torch.matmul(B,P_opt)),0,1)))))                       \n",
            "            ^\n",
            "\n",
            "  @jit\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"Sum_rate_loss_tensor\" was compiled in object mode without forceobj=True.\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 22:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "    temp_tensor_pred_check = temp_tensor_pred\n",
            "    for rows in prange(len(H_eff_tensor)):\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n",
            "/usr/local/lib/python3.7/dist-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
            "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
            "\n",
            "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
            "\n",
            "File \"<ipython-input-37-a0bf00e11f4d>\", line 22:\n",
            "def Sum_rate_loss_tensor(H_eff_tensor,P_label_tensor,P_pred_tensor,NoisePower,P_t,alpha):\n",
            "    <source elided>\n",
            "    temp_tensor_pred_check = temp_tensor_pred\n",
            "    for rows in prange(len(H_eff_tensor)):\n",
            "    ^\n",
            "\n",
            "  state.func_ir.loc))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------0\tbatch-loss: 23.3080\tval-loss: 1587.0752\n",
            "-----------1\tbatch-loss: 20.6919\tval-loss: 844.1158\n",
            "-----------2\tbatch-loss: 5.1165\tval-loss: 351.9540\n",
            "-----------3\tbatch-loss: 140.3604\tval-loss: 278.8418\n",
            "-----------4\tbatch-loss: 2.6908\tval-loss: 254.0633\n",
            "-----------5\tbatch-loss: 75.6562\tval-loss: 235.2030\n",
            "-----------6\tbatch-loss: 34.8061\tval-loss: 223.9452\n",
            "-----------7\tbatch-loss: 268.6542\tval-loss: 220.3216\n",
            "-----------8\tbatch-loss: 293.5082\tval-loss: 219.3692\n",
            "-----------9\tbatch-loss: 67.2894\tval-loss: 219.1282\n",
            "-----------10\tbatch-loss: 0.0319\tval-loss: 219.0375\n",
            "-----------11\tbatch-loss: 440.9828\tval-loss: 218.6526\n",
            "-----------12\tbatch-loss: 4.2648\tval-loss: 218.4372\n",
            "-----------13\tbatch-loss: 57.8415\tval-loss: 217.9315\n",
            "-----------14\tbatch-loss: 2283.8210\tval-loss: 217.3084\n",
            "-----------15\tbatch-loss: 0.5373\tval-loss: 216.4729\n",
            "-----------16\tbatch-loss: 14.7188\tval-loss: 215.3271\n",
            "-----------17\tbatch-loss: 87.7537\tval-loss: 213.3332\n",
            "-----------18\tbatch-loss: 461.2231\tval-loss: 209.7464\n",
            "-----------19\tbatch-loss: 34.5038\tval-loss: 202.4864\n",
            "-----------20\tbatch-loss: 2004.7618\tval-loss: 196.0902\n",
            "-----------21\tbatch-loss: 10.8854\tval-loss: 190.7686\n",
            "-----------22\tbatch-loss: 489.5921\tval-loss: 187.6281\n",
            "-----------23\tbatch-loss: 87.9058\tval-loss: 186.6188\n",
            "-----------24\tbatch-loss: 446.0435\tval-loss: 185.3413\n",
            "-----------25\tbatch-loss: 90.4684\tval-loss: 184.6896\n",
            "-----------26\tbatch-loss: 202.9754\tval-loss: 184.2729\n",
            "-----------27\tbatch-loss: 558.4941\tval-loss: 183.2859\n",
            "-----------28\tbatch-loss: 4.5402\tval-loss: 182.6167\n",
            "-----------29\tbatch-loss: 74.7194\tval-loss: 182.6262\n",
            "-----------30\tbatch-loss: 70.3216\tval-loss: 181.8371\n",
            "-----------31\tbatch-loss: 477.4840\tval-loss: 180.3939\n",
            "-----------32\tbatch-loss: 0.0012\tval-loss: 179.6724\n",
            "-----------33\tbatch-loss: 2.6849\tval-loss: 179.5692\n",
            "-----------34\tbatch-loss: 69.9158\tval-loss: 178.5469\n",
            "-----------35\tbatch-loss: 28.0091\tval-loss: 176.4574\n",
            "-----------36\tbatch-loss: 338.1294\tval-loss: 175.6317\n",
            "-----------37\tbatch-loss: 190.5620\tval-loss: 173.9332\n",
            "-----------38\tbatch-loss: 134.4517\tval-loss: 172.6975\n",
            "-----------39\tbatch-loss: 12.1501\tval-loss: 171.5955\n",
            "-----------40\tbatch-loss: 71.0506\tval-loss: 169.8710\n",
            "-----------41\tbatch-loss: 1.0866\tval-loss: 169.3987\n",
            "-----------42\tbatch-loss: 0.1756\tval-loss: 167.9546\n",
            "-----------43\tbatch-loss: 40.6456\tval-loss: 167.5518\n",
            "-----------44\tbatch-loss: 453.3265\tval-loss: 166.8962\n",
            "-----------45\tbatch-loss: 5.1965\tval-loss: 166.9026\n",
            "-----------46\tbatch-loss: 1.6034\tval-loss: 166.4650\n",
            "-----------47\tbatch-loss: 2.5031\tval-loss: 165.5694\n",
            "-----------48\tbatch-loss: 9.8959\tval-loss: 165.2070\n",
            "-----------49\tbatch-loss: 60.8940\tval-loss: 165.2809\n",
            "-----------50\tbatch-loss: 48.4144\tval-loss: 164.7331\n",
            "-----------51\tbatch-loss: 51.4687\tval-loss: 164.7126\n",
            "-----------52\tbatch-loss: 78.3553\tval-loss: 165.1205\n",
            "-----------53\tbatch-loss: 11.0934\tval-loss: 164.8922\n",
            "-----------54\tbatch-loss: 78.1270\tval-loss: 164.2866\n",
            "-----------55\tbatch-loss: 102.6092\tval-loss: 164.6513\n",
            "-----------56\tbatch-loss: 498.3637\tval-loss: 164.2198\n",
            "-----------57\tbatch-loss: 45.7013\tval-loss: 164.2123\n",
            "-----------58\tbatch-loss: 51.3657\tval-loss: 164.3097\n",
            "-----------59\tbatch-loss: 0.4785\tval-loss: 163.8452\n",
            "-----------60\tbatch-loss: 0.8373\tval-loss: 164.4169\n",
            "-----------61\tbatch-loss: 20.0764\tval-loss: 163.5356\n",
            "-----------62\tbatch-loss: 7.5974\tval-loss: 163.8506\n",
            "-----------63\tbatch-loss: 406.7550\tval-loss: 163.3435\n",
            "-----------64\tbatch-loss: 3.7316\tval-loss: 163.8023\n",
            "-----------65\tbatch-loss: 7.9574\tval-loss: 163.8858\n",
            "-----------66\tbatch-loss: 9.1466\tval-loss: 163.6990\n",
            "-----------67\tbatch-loss: 136.9586\tval-loss: 163.0707\n",
            "-----------68\tbatch-loss: 23.0337\tval-loss: 163.1816\n",
            "-----------69\tbatch-loss: 588.2107\tval-loss: 163.0855\n",
            "-----------70\tbatch-loss: 185.4229\tval-loss: 163.0741\n",
            "-----------71\tbatch-loss: 258.8492\tval-loss: 163.4918\n",
            "-----------72\tbatch-loss: 160.5903\tval-loss: 163.0682\n",
            "-----------73\tbatch-loss: 21.0957\tval-loss: 163.3227\n",
            "-----------74\tbatch-loss: 7.0071\tval-loss: 163.4030\n",
            "-----------75\tbatch-loss: 9.2326\tval-loss: 163.9366\n",
            "-----------76\tbatch-loss: 49.4720\tval-loss: 164.8048\n",
            "-----------77\tbatch-loss: 127.9918\tval-loss: 164.8510\n",
            "-----------78\tbatch-loss: 42.1219\tval-loss: 164.2933\n",
            "-----------79\tbatch-loss: 7.8281\tval-loss: 163.6519\n",
            "-----------80\tbatch-loss: 10.9415\tval-loss: 164.1559\n",
            "-----------81\tbatch-loss: 387.6097\tval-loss: 163.5787\n",
            "-----------82\tbatch-loss: 144.1440\tval-loss: 163.8976\n",
            "-----------83\tbatch-loss: 54.5678\tval-loss: 163.3728\n",
            "-----------84\tbatch-loss: 3.7845\tval-loss: 163.5753\n",
            "-----------85\tbatch-loss: 0.6353\tval-loss: 163.7632\n",
            "-----------86\tbatch-loss: 64.5804\tval-loss: 163.9184\n",
            "-----------87\tbatch-loss: 415.0795\tval-loss: 163.7806\n",
            "-----------88\tbatch-loss: 74.7166\tval-loss: 162.9951\n",
            "-----------89\tbatch-loss: 228.9297\tval-loss: 162.9006\n",
            "-----------90\tbatch-loss: 6.8135\tval-loss: 164.0140\n",
            "-----------91\tbatch-loss: 25.7202\tval-loss: 162.9275\n",
            "-----------92\tbatch-loss: 13.2622\tval-loss: 163.0211\n",
            "-----------93\tbatch-loss: 8.3243\tval-loss: 162.9953\n",
            "-----------94\tbatch-loss: 0.0260\tval-loss: 163.4546\n",
            "-----------95\tbatch-loss: 121.8072\tval-loss: 163.7731\n",
            "-----------96\tbatch-loss: 102.0314\tval-loss: 163.5294\n",
            "-----------97\tbatch-loss: 122.8065\tval-loss: 163.8834\n",
            "-----------98\tbatch-loss: 350.7347\tval-loss: 163.6999\n",
            "-----------99\tbatch-loss: 10.3710\tval-loss: 163.7821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "Rd3JASV61Xzp",
        "outputId": "c43cf47c-2595-4d45-b3a7-3ebc8bcd9dfe"
      },
      "source": [
        "\n",
        "#%% print batch loss graph\n",
        "'''\n",
        "for items in range(len(batch_loss_all)):\n",
        "    \n",
        "    batch_loss_all[items] = batch_loss_all[items].cpu().detach().numpy()\n",
        "for items in range(len(val_loss_all)):\n",
        "    val_loss_all[items] = val_loss_all[items].cpu().detach().numpy()\n",
        "'''\n",
        "#%%\n",
        "\n",
        "\n",
        "plt.plot(list(range(len(batch_loss_all))),(batch_loss_all))\n",
        "plt.xlabel('batches')\n",
        "plt.ylabel('batch_loss')\n",
        "plt.title('batch_loss_plot')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(list(range(len(val_loss_all))),val_loss_all)\n",
        "plt.xlabel('Epoches')\n",
        "plt.ylabel('validation_loss')\n",
        "plt.title('validation_loss_plot')\n",
        "plt.show()\n",
        "\n",
        "#%%\n",
        "print(y_hat_max)\n",
        "\n",
        "#%%\n",
        "print(y_max)\n",
        "#%%\n",
        "print(val_loss_min)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.5788e-04, 4.3809e-04, 7.1028e-03, 5.8109e-07, 6.2011e-07, 6.5016e-03]])\n",
            "tensor([[0.2161, 0.8314, 0.0143, 0.0086, 0.0119, 0.9999]])\n",
            "tensor(162.9006)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}